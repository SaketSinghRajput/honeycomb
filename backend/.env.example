# ============================================================================
# Model Configuration
# ============================================================================

# Directory to store downloaded models (default: ./models)
MODELS_DIR=./models

# Whisper model size: tiny, base, small, medium, large (default: base)
# Smaller models are faster but less accurate; larger models are slower but more accurate
WHISPER_MODEL_NAME=base

# DistilBERT model for scam detection (default: facebook/bart-large-mnli)
# Uses zero-shot learning for scam classification
DISTILBERT_MODEL_NAME=facebook/bart-large-mnli

# spaCy NER model for entity extraction (default: en_core_web_sm)
# Download with: python -m spacy download en_core_web_sm
SPACY_MODEL_NAME=en_core_web_sm

# Coqui TTS model (default: tts_models/en/ek1/tacotron2)
# Indian English voice suitable for scam honeypot
TTS_MODEL_NAME=tts_models/en/ek1/tacotron2

# LLM model for conversational engagement (default: microsoft/phi-2)
LLM_MODEL_NAME=microsoft/phi-2

# ============================================================================
# LLM Configuration (Local vs API)
# ============================================================================

# Use external LLM API instead of loading locally (default: false)
# Set to true for resource-constrained environments
LLM_USE_API=false

# LLM API base URL (e.g., http://localhost:8001/v1 for local Ollama)
# Only used if LLM_USE_API=true
LLM_API_BASE_URL=http://localhost:8001/v1

# LLM API key for authentication (e.g., sk-xxxx for OpenAI)
# Only used if LLM_USE_API=true
LLM_API_KEY=sk-test

# Maximum tokens for LLM response (default: 150)
LLM_MAX_TOKENS=150

# Temperature for LLM sampling: 0.0-1.0 (default: 0.7)
# Lower values = more deterministic, higher values = more creative
LLM_TEMPERATURE=0.7

# ============================================================================
# Agent Configuration
# ============================================================================

# Maximum conversation turns to keep in memory (default: 5)
# Older turns are discarded to limit context length
AGENT_MAX_MEMORY_TURNS=5

# Conversation termination keywords (JSON array)
# Conversation ends if agent response contains these keywords
# Use a JSON array so pydantic-settings can decode it from .env
AGENT_TERMINATION_KEYWORDS=["goodbye", "thank you", "disconnect"]

# ============================================================================
# TTS Configuration
# ============================================================================

# TTS language code (default: en)
TTS_LANGUAGE=en

# TTS speaker ID or name (default: p225)
# Different speakers provide different voice characteristics
TTS_SPEAKER=p225

# ============================================================================
# API Configuration
# ============================================================================

# API host address (default: 0.0.0.0 for all interfaces)
# Set to 127.0.0.1 for localhost only (development)
API_HOST=0.0.0.0

# API port (default: 8000)
API_PORT=8000

# Number of Uvicorn worker processes (default: 1)
# For production: set to CPU_CORES - 1 (e.g., 3 for t3.large with 4 cores)
API_WORKERS=1

# CORS allowed origins (JSON array)
# For production: set to specific domains, e.g., ["https://example.com","https://app.example.com"]
# Use ["*"] to allow all origins (development only)
CORS_ORIGINS=["*"]

# ============================================================================
# API Authentication
# ============================================================================
# API key used to protect sensitive endpoints. Change this in production!
API_SECRET_KEY=sk_test_123456789

# ============================================================================
# Runtime Configuration
# ============================================================================

# Device for model inference: cpu, cuda (default: auto-detects)
# Set to cpu for CPU-only instances
# Set to cuda for GPU instances (requires NVIDIA drivers and CUDA)
DEVICE=auto

# Logging level: DEBUG, INFO, WARNING, ERROR, CRITICAL (default: INFO)
LOG_LEVEL=INFO

# Demo mode: if true, uses mock responses without loading models (default: false)
# Useful for testing, CI/CD, or development without model resources
DEMO_MODE=false

# ============================================================================
# Examples
# ============================================================================

# === CPU Deployment (t3.large) ===
# Uncomment these lines for CPU-only instance:
# DEVICE=cpu
# API_WORKERS=2
# LLM_USE_API=false
# WHISPER_MODEL_NAME=tiny

# === GPU Deployment (g4dn.xlarge) ===
# Uncomment these lines for GPU instance:
# DEVICE=cuda
# API_WORKERS=4
# LLM_USE_API=false
# WHISPER_MODEL_NAME=base

# === Resource-Constrained Deployment ===
# Uncomment these lines for minimal resources (demo/development):
# DEVICE=cpu
# API_WORKERS=1
# DEMO_MODE=true

# === External LLM API (e.g., Ollama on local machine) ===
# Uncomment these lines to use external LLM instead of loading locally:
# LLM_USE_API=true
# LLM_API_BASE_URL=http://localhost:8001/v1
# LLM_API_KEY=sk-test

# ============================================================================
# GUVI Hackathon Callback Configuration
# ============================================================================
GUVI_CALLBACK_URL=https://hackathon.guvi.in/api/updateHoneyPotFinalResult
GUVI_CALLBACK_ENABLED=true
GUVI_CALLBACK_TIMEOUT=10
AGENT_MIN_TURNS_FOR_CALLBACK=3
AGENT_MAX_TURNS_FOR_CALLBACK=15
